{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "We will be using the same dataset acquired from two different endpoints.\n",
    "- OpenML: More friendly experience to use with sklearn\n",
    "- Tensorflow Datasets: Easier to use within TensorFlow for deep learning approaches.\n",
    "\n",
    "Despite the multiple sources, our data is the exact same across both sources, and these visualizations\n",
    "represent both sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Dataset for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], with_info=True, as_supervised=True, shuffle_files=True)\n",
    "\n",
    "example = train_ds.take(1)\n",
    "\n",
    "# Showcasing single handwritten digit image in numpy's NdArray and its\n",
    "# truth label from the training dataset.\n",
    "for image, label in tfds.as_numpy(example):  # example is (image, label)\n",
    "  print(image.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize 9 digits from the training datasets\n",
    "\n",
    "- Plotting all 784 pixels into 9 (28x28) square images representing the first 9 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Naive Classification Approaches\n",
    "\n",
    "The following approaches are naive because each of these models were first built with arbitrarily selected parameters based on previous experience with building models on other datasets. If the initial performance is well, then we pursue hyperparameter training\n",
    "\n",
    "- Logistic Regression Classifier\n",
    "- Naive Bayes Classifier\n",
    "- Decision Trees Classifier\n",
    "- K-Nearest Neighbors Classifier\n",
    "- Support Vector Machines Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset for SKLearn & Initial Naive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mnist.data))\n",
    "mnist.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Subset of Data\n",
    "\n",
    "- Plotting all 784 pixels in a 28 x 28 subplot for the first 6 images from the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = mnist.data.to_numpy()\n",
    "plt.subplot(431)\n",
    "plt.imshow((image[0].reshape(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(432)\n",
    "plt.imshow(image[1].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(433)\n",
    "plt.imshow(image[3].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(434)\n",
    "plt.imshow(image[4].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(435)\n",
    "plt.imshow(image[5].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(436)\n",
    "plt.imshow(image[6].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Up Data into Training & Test\n",
    "\n",
    "We are splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=10_000, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (divide every pixel value by 255)\n",
    "X_train_scaled = X_train / 255.0\n",
    "X_test_scaled = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Development\n",
    "\n",
    "- Models are built with arbitrary parameters based on prior experience with building models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 4: Create a logistic regression model\n",
    "# Random State is given specifically for reproducability\n",
    "lr = LogisticRegression(multi_class='multinomial', random_state=42)\n",
    "\n",
    "# Step 5: Perform hyperparameter tuning using GridSearchCV\n",
    "lr_pg = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']  # Different solvers\n",
    "}  # Hyperparameter grid\n",
    "lr_gs = GridSearchCV(lr, lr_pg, cv=5, verbose=1, n_jobs=-1)\n",
    "lr_gs.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "lr_bp = lr_gs.best_params_\n",
    "\n",
    "# Step 6: Evaluate the model on the testing set\n",
    "lr_bm = lr_gs.best_estimator_\n",
    "lr_acc = lr_gs.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lr_gs_res = pd.DataFrame(lr_gs.cv_results_)\n",
    "lr_gs_res = lr_gs_res.sort_values(by='rank_test_score')\n",
    "\n",
    "lr_gs_res.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Best Model\n",
    "\n",
    "- Showcase best parameters\n",
    "- Showcase accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(lr_gs.best_params_)\n",
    "# Same as cross-entropy loss which is the recommended function to use\n",
    "# for multi-nomial class label classification problems\n",
    "print(\"Accuracy via log_loss\", lr_acc)\n",
    "lr_y_pred = lr_bm.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "lr_class_rep = classification_report(y_test, lr_y_pred)\n",
    "print(lr_class_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Classifier\n",
    "\n",
    "There are no hyperparamters / parameters to tune.\n",
    "\n",
    "Naive Bayes gives pretty low accuracy because it consideres all the probablities as independent of each other (hence-naive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=10_000, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (divide every pixel value by 255)\n",
    "X_train_scaled = X_train / 255.0\n",
    "X_test_scaled = X_test / 255.0\n",
    "\n",
    "# Step 4: Create a Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Step 5: Perform hyperparameter tuning (No hyperparameters to tune for Naive Bayes)\n",
    "# You can skip this step for Naive Bayes\n",
    "\n",
    "# Step 6: Fit the model\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 7: Evaluate the model on the testing set\n",
    "nb_acc = nb.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy via log_loss\", nb_acc)\n",
    "nb_y_pred = nb.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "nb_class_rep = classification_report(y_test, nb_y_pred)\n",
    "print(nb_class_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 3: Create a Decision Trees model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Step 4: Perform hyperparameter tuning using GridSearchCV\n",
    "dt_pg = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}  # Hyperparameter grid\n",
    "\n",
    "dt_gs = GridSearchCV(dt, dt_pg, cv=5, verbose=1, n_jobs=-1)\n",
    "dt_gs.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "dt_bp = dt_gs.best_params_\n",
    "\n",
    "# Step 5: Evaluate the model on the testing set\n",
    "dt_bm = dt_gs.best_estimator_\n",
    "dt_acc = dt_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dt_gs_res = pd.DataFrame(dt_gs.cv_results_)\n",
    "dt_gs_res = dt_gs_res.sort_values(by='rank_test_score')\n",
    "\n",
    "dt_gs_res.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Best Model\n",
    "\n",
    "- Showcase best parameters\n",
    "- Showcase accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Best Parameters\", dt_bp)\n",
    "print(\"Accuracy via log_loss\", dt_acc)\n",
    "dt_y_pred = dt_bm.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "dt_class_rep = classification_report(y_test, dt_y_pred)\n",
    "print(dt_class_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Step 4: Create a KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Step 5: Perform hyperparameter tuning using GridSearchCV\n",
    "knn_pg = {\n",
    "    'n_neighbors': range(98, 100, 2),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}  # Hyperparameter grid\n",
    "\n",
    "knn_gs = GridSearchCV(knn, knn_pg, cv=5, verbose=10, n_jobs=-1)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "knn_bp = knn_gs.best_params_\n",
    "\n",
    "# Step 6: Evaluate the model on the testing set\n",
    "knn_bm = knn_gs.best_estimator_\n",
    "knn_acc = knn_gs.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "knn_gs_res = pd.DataFrame(knn_gs.cv_results_)\n",
    "knn_gs_res = knn_gs_res.sort_values(by='rank_test_score')\n",
    "\n",
    "knn_gs_res.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Best Model\n",
    "\n",
    "- Showcase best parameters\n",
    "- Showcase accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN Best Parameters\", knn_bp)\n",
    "print(\"Accuracy via log_loss\", knn_acc)\n",
    "knn_y_pred = knn_bm.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "knn_class_rep = classification_report(y_test, knn_y_pred)\n",
    "print(knn_class_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "base_svm = SVC(C=1.0, kernel='rbf')\n",
    "base_svm.fit(X_train, y_train)\n",
    "acc = base_svm.score(X_test,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Step 4: Create a SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Step 5: Perform hyperparameter tuning using GridSearchCV\n",
    "svm_pg = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']  # Kernel coefficient\n",
    "}  # Hyperparameter grid\n",
    "\n",
    "svm_gs = GridSearchCV(svm, svm_pg, cv=5, verbose=1, n_jobs=-1)\n",
    "svm_gs.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "svm_bp = svm_gs.best_params_\n",
    "\n",
    "# Step 6: Evaluate the model on the testing set\n",
    "svm_bm = svm_gs.best_estimator_\n",
    "svm_acc = svm_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "svm_gs_res = pd.DataFrame(svm_gs.cv_results_)\n",
    "svm_gs_res = svm_gs_res.sort_values(by='rank_test_score')\n",
    "\n",
    "svm_gs_res.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Best Model\n",
    "\n",
    "- Showcase best parameters\n",
    "- Showcase accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM Best Parameters\", svm_bp)\n",
    "print(\"Accuracy via log_loss\", svm_acc)\n",
    "svm_y_pred = svm_bp.predict(X_test_scaled)\n",
    "\n",
    "# Generate classification report\n",
    "svm_class_rep = classification_report(y_test, svm_y_pred)\n",
    "print(svm_class_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
